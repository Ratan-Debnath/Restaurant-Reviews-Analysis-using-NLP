{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "restaurant-reviews-analysis-using-nlp.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LwxOHrg_lVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#help from nltk.org, scikit-learn.org, geeks for geeks.\n",
        "#Step 1: Setup Environment\n",
        "# Importing libraries and dataset with setting delimiter as ‘\\t’ as columns are separated\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SLL_Ufi_5EH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "211a0a40-bb73-4210-a602-56eabeac60b2"
      },
      "source": [
        "#Step 2: Text Cleaning or Preprocessing\n",
        "import re      #library to clean data\n",
        "import nltk    #Natural Language Tool kit\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords  #remove stopwords\n",
        "#for Stemming propose\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDYLH2TL_-un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize empty array to append clean text\n",
        "corpus =[]\n",
        "\n",
        "#1000(reviews) rows to clean \n",
        "for i in range(0,1000):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])  ## column : \"Review\", row ith\n",
        "  #convert all cases to lower cases\n",
        "  review = review.lower()\n",
        "  #split to array(default delimiter is \" \")\n",
        "  review = review.split()\n",
        "  # creating PorterStemmer object to  take main stem of each word \n",
        "  ps = PorterStemmer()\n",
        "  review = [ps.stem(word) for word in review\n",
        "            if not word in set(stopwords.words('english'))]\n",
        "  #rejoin all string array elements to create back into a string     \n",
        "  review = ' '.join(review)\n",
        "  #append each string to create array of clean text    \n",
        "  corpus.append(review)\n",
        "\n",
        "#print(review)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i_wl9ZODtd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 3: Tokenization, involves splitting sentences and words from the body of the text.\n",
        "#Step 4.Create Bag of Words model\n",
        "from sklearn.feature_extraction.text import  CountVectorizer\n",
        "#extract max 1500 feature\n",
        "#\"max_features\" is attribute to experiment with to get better results \n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "\n",
        "# X contains corpus (dependent variable) \n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "#Y contains answer if review is positive or negative\n",
        "y = dataset.iloc[:,1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RrwGsPFNjCh",
        "colab_type": "text"
      },
      "source": [
        "Description of the dataset:\n",
        "\n",
        "*   Columns seperated by \\t (tab space)\n",
        "*   First column is about reviews of people\n",
        "*   In second column, 0 is for negative review and 1 is for positive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzK9sq5BPBLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 5: Spliting Corpus into Training and Test set.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar2LIg8yR4Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 6: Fitting a Predictive Model(random forest)\n",
        "Since Random fored is ensemble model (made of many trees) from sklearn.ensemble, import RandomForestClassifier class\n",
        "With 501 tree or “n_estimators” and criterion as ‘entropy’\n",
        "Fit the model via .fit() method with attributes X_train and y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80RoEIUlZMkp",
        "colab_type": "text"
      },
      "source": [
        "#Step 6: Fitting a Predictive Model(random forest)\n",
        "\n",
        "*  Since Random fored is ensemble model(made of many trees) from sklearn.ensemble, import RandomForestClassifier class\n",
        "*   With 501 tree or “n_estimators” and criterion as ‘entropy’\n",
        "*   Fit the model via .fit() method with attributes X_train and y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz-J3LwjZwdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8bf60c32-deff-4b43-9fb7-5eda739eeec3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# n_estimators can be said as number of trees, experiment with n_estimators to get better results\n",
        "model = RandomForestClassifier(n_estimators = 501, criterion = 'entropy') \n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=501,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frZ1t2Csbo-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "89ab3294-ea62-4223-9695-35f445505bae"
      },
      "source": [
        "#Step 7: Predicting the test set results\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW_QrKugmGjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b9dc04c9-1680-4c68-b31c-76d39193504b"
      },
      "source": [
        "#Step 8:performance of a classification model using Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "results = confusion_matrix(y_test, y_pred) \n",
        "print('Confusion Matrix :')\n",
        "results"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[107,   9],\n",
              "       [ 53,  81]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB2i15bioclX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ffbb9c14-605f-4f39-f1d2-8e80c071e36d"
      },
      "source": [
        "from sklearn.metrics import  accuracy_score, classification_report\n",
        "print('Accuracy Score :',accuracy_score(y_test, y_pred))\n",
        "print('Report : ')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score : 0.752\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.92      0.78       116\n",
            "           1       0.90      0.60      0.72       134\n",
            "\n",
            "    accuracy                           0.75       250\n",
            "   macro avg       0.78      0.76      0.75       250\n",
            "weighted avg       0.79      0.75      0.75       250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}